import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
object rddjoin {

  def main(args: Array[String]) {
    var sparkConf = new SparkConf().setAppName("samplejob").setMaster("local[*]")
    var sc: org.apache.spark.SparkContext = new org.apache.spark.SparkContext(sparkConf);
    
    val file1=sc.textFile("/test/test")
     val file2=sc.textFile("/test/test1")
     val rdd1=file1.map( x=> (x.split("")(0),x))
     val rdd2=file2.map( x=> (x.split("")(0),x))
     val res=rdd1.leftOuterJoin(rdd2)
     res.saveAsTextFile("/test/test2")
  }
}
